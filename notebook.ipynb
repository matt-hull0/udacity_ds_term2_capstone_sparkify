{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# # OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Popol Vuh', auth='Logged In', firstName='Shlok', gender='M', itemInSession=278, lastName='Johnson', length=524.32934, level='paid', location='Dallas-Fort Worth-Arlington, TX', method='PUT', page='NextSong', registration=1533734541000, sessionId=22683, song='Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent='\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='1749042')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Read in full sparkify dataset\n",
    "event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "raw_df = spark.read.json(event_data)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+-------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent| userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+-------+\n",
      "|  Popol Vuh|Logged In|    Shlok|     M|          278| Johnson|524.32934| paid|Dallas-Fort Worth...|   PUT|NextSong|1533734541000|    22683|Ich mache einen S...|   200|1538352001000|\"Mozilla/5.0 (Win...|1749042|\n",
      "|Los Bunkers|Logged In|  Vianney|     F|            9|  Miller|238.39302| paid|San Francisco-Oak...|   PUT|NextSong|1537500318000|    20836|         MiÃÂ©ntele|   200|1538352002000|\"Mozilla/5.0 (Mac...|1563081|\n",
      "|       Lush|Logged In|     Vina|     F|          109|  Bailey|140.35546| paid|            Hilo, HI|   PUT|NextSong|1536414505000|     4593|           Baby Talk|   200|1538352002000|Mozilla/5.0 (Maci...|1697168|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "## Initial overview using ydata-profiling library"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "report = ProfileReport(df, title=\"Profiling Report\",infer_dtypes=False,\n",
    "                interactions=None,\n",
    "                missing_diagrams=None,\n",
    "                correlations={\"auto\": {\"calculate\": False},\n",
    "                              \"pearson\": {\"calculate\": True},\n",
    "                              \"spearman\": {\"calculate\": True}})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema information - taken from https://www.kaggle.com/code/yukinagae/sparkify-project-churn-prediction\n",
    "\n",
    "artist: Artist name (ex. Daft Punk) \\\n",
    "auth: User authentication status (ex. Logged) \\\n",
    "firstName: User first name (ex. Colin) \\\n",
    "gender: Gender (ex. F or M) \\\n",
    "itemInSession: Item count in a session (ex. 52) \\\n",
    "lastName: User last name (ex. Freeman) \\\n",
    "length: Length of song (ex. 223.60771) \\\n",
    "level: User plan (ex. paid) \\\n",
    "location: User's location (ex. Bakersfield \\)\n",
    "method: HTTP method (ex. PUT) \\\n",
    "page: Page name (ex. NextSong) \\\n",
    "registration: Registration timestamp (unix timestamp) (ex. 1538173362000) \\\n",
    "sessionId: Session ID (ex. 29) \\\n",
    "song: Song (ex. Harder Better Faster Stronger) \\\n",
    "status: HTTP status (ex. 200) \\\n",
    "ts: Event timestamp(unix timestamp) (ex. 1538352676000) \\\n",
    "userAgent: User's browswer agent (ex. Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0) \\\n",
    "userId: User ID (ex. 30) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|page                     |\n",
      "+-------------------------+\n",
      "|Cancel                   |\n",
      "|Submit Downgrade         |\n",
      "|Thumbs Down              |\n",
      "|Home                     |\n",
      "|Downgrade                |\n",
      "|Roll Advert              |\n",
      "|Logout                   |\n",
      "|Save Settings            |\n",
      "|Cancellation Confirmation|\n",
      "|About                    |\n",
      "|Submit Registration      |\n",
      "|Settings                 |\n",
      "|Login                    |\n",
      "|Register                 |\n",
      "|Add to Playlist          |\n",
      "|Add Friend               |\n",
      "|NextSong                 |\n",
      "|Thumbs Up                |\n",
      "|Help                     |\n",
      "|Upgrade                  |\n",
      "|Error                    |\n",
      "|Submit Upgrade           |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the unique pages are:\n",
    "df.select(\"page\").distinct().show(23, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations from the ydata-profile report:\n",
    "\n",
    " - 26_259_199 rows\n",
    " - 18 columns\n",
    " - 14_710_249\n",
    "\n",
    "- no duplicate rows\n",
    "\n",
    "  ### Columns:\n",
    "\n",
    "  - artist - missing values, to be expected when the interaction doesn't involve a song (e.g. logging in)\n",
    "  - firstName - missing 778_479 values (note originally thought was 14_562_194 values but ydata report is confusing / bugged - see below)\n",
    "  - gender - missing 778_479 (3%) of values\n",
    "  - lastName - missing 778_479 values (note originally thought was 12_948_838 values but ydata report is confusing / bugged - see below)\n",
    "  - level - 2 distinct values (paid / free), no missing - the majority (78%) are paying users\n",
    "  - location - has missing values. Most popular locations seem to be in USA\n",
    "  - page - 22 distinct values, none missing. Most visited page is \"NextSong\"\n",
    "  - registration - no missing values, but some at 0 which will need to be checked / cleaned up\n",
    "  - status - none missing, 3 distinct values: 200 (OK), 307 (Temporary Redirect), 404 (NOT FOUND)\n",
    "  - ts - none missing, don't have the same \"zeros\" issue as registrations\n",
    "  - userId - none missing, but one very common value (1261737). More checks might be required (e.g. for empty string)\n",
    "\n",
    "\n",
    " side note for the negotiating team - our most played artists are Kings of Leon and Coldplay so don't lose the rights to play their songs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent| userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "|  null|Logged Out|     null|  null|           87|    null|  null| paid|    null|   GET| Home|        null|     8615|null|   200|1538352008000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|     7433|null|   307|1538352041000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            4|    null|  null| free|    null|   GET| Home|        null|    25003|null|   200|1538352182000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            2|    null|  null| free|    null|   GET| Home|        null|     9930|null|   200|1538352254000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            3|    null|  null| free|    null|   PUT|Login|        null|     9930|null|   307|1538352255000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|    23471|null|   307|1538352259000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           44|    null|  null| free|    null|   GET| Home|        null|     6317|null|   200|1538352278000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           43|    null|  null| free|    null|   GET| Home|        null|    22951|null|   200|1538352361000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           44|    null|  null| free|    null|   GET| Home|        null|    22951|null|   200|1538352365000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           95|    null|  null| paid|    null|   GET| Home|        null|     6071|null|   200|1538352404000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           96|    null|  null| paid|    null|   PUT|Login|        null|     6071|null|   307|1538352405000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          137|    null|  null| paid|    null|   GET| Home|        null|     9422|null|   200|1538352461000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           68|    null|  null| paid|    null|   GET| Home|        null|    21447|null|   200|1538352464000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           69|    null|  null| paid|    null|   PUT|Login|        null|    21447|null|   307|1538352465000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          138|    null|  null| paid|    null|   GET| Home|        null|     9422|null|   200|1538352481000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          139|    null|  null| paid|    null|   PUT|Login|        null|     9422|null|   307|1538352482000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          279|    null|  null| paid|    null|   GET| Home|        null|     9659|null|   200|1538352501000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           29|    null|  null| free|    null|   GET| Home|        null|    10467|null|   200|1538352507000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          150|    null|  null| paid|    null|   GET| Home|        null|    13098|null|   200|1538352520000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          280|    null|  null| paid|    null|   GET| Home|        null|     9659|null|   200|1538352542000|     null|1261737|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understand the many missing values in \"firstName\" / \"lastName\" column\n",
    "# Code inspired by: https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "\n",
    "df_missing_first_or_last_name = df.where(\n",
    "    (f.col(\"firstName\")==\"\") |\n",
    "    (f.col(\"firstName\").isNull()) |\n",
    "    (f.col(\"lastName\")==\"\") |\n",
    "    (f.col(\"lastName\").isNull())\n",
    "    )\n",
    "df_missing_first_or_last_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "|Logged Out|\n",
      "|     Guest|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what are the auth states for users missing names:\n",
    "df_missing_first_or_last_name.select(\"auth\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userId|\n",
      "+-------+\n",
      "|1261737|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# whilst users are always Logged Out or a guest (also not sure what the difference between those is), there are both levels (paid/free) as well as for the same userId: 1261737, which is the one we commented on having many more rows than all the other users. Keep digging:\n",
    "df_missing_first_or_last_name.select(\"userId\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               page|\n",
      "+-------------------+\n",
      "|               Home|\n",
      "|              About|\n",
      "|Submit Registration|\n",
      "|              Login|\n",
      "|           Register|\n",
      "|               Help|\n",
      "|              Error|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So actually that is the only userId causing nulls in many columns \n",
    "\n",
    "df_missing_first_or_last_name.select(\"page\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This user never goes to the \"Upgrade\" / \"Submit Upgrade\" page, so is odd they can have the \"paid\" level in the data. It could be some trial scheme sparkify has or similar, but as we don't know and since the \"user\" (it could be many different users that just get given the same id when logged out / a guest) is missing a lot of information about them (both personally and what they are doing on the platform e.g. how many songs listening to etc - if any), we will remove this user from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# But there are also more first names missing than last names:\n",
    "df_missing_first_got_last = df_missing_first_or_last_name = df.where(\n",
    "    (f.col(\"firstName\").isNull()) &\n",
    "    (f.col(\"lastName\").isNotNull())\n",
    "    )\n",
    "df_missing_first_got_last.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstName|lastName|\n",
      "+---------+--------+\n",
      "|   778479|  778479|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code inspired by: https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "\n",
    "df.select([f.count(f.when(f.isnan(c) | f.col(c).isNull(), c)).alias(c) for c in [\"firstName\", \"lastName\"]]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So there is not a different number of nulls in each of the columns. This is different to the values from ydata-profiling tool, which clearly has some sort of bug to be resolved.\n",
    "#(reviewed and found an issue already exists that covers it: https://github.com/ydataai/ydata-profiling/issues/1429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "|artist|      auth|firstName|gender|itemInSession|lastName|length|level|location|method| page|registration|sessionId|song|status|           ts|userAgent| userId|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "|  null|Logged Out|     null|  null|           87|    null|  null| paid|    null|   GET| Home|        null|     8615|null|   200|1538352008000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|     7433|null|   307|1538352041000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            4|    null|  null| free|    null|   GET| Home|        null|    25003|null|   200|1538352182000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            2|    null|  null| free|    null|   GET| Home|        null|     9930|null|   200|1538352254000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            3|    null|  null| free|    null|   PUT|Login|        null|     9930|null|   307|1538352255000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|            0|    null|  null| free|    null|   PUT|Login|        null|    23471|null|   307|1538352259000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           44|    null|  null| free|    null|   GET| Home|        null|     6317|null|   200|1538352278000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           43|    null|  null| free|    null|   GET| Home|        null|    22951|null|   200|1538352361000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           44|    null|  null| free|    null|   GET| Home|        null|    22951|null|   200|1538352365000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           95|    null|  null| paid|    null|   GET| Home|        null|     6071|null|   200|1538352404000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           96|    null|  null| paid|    null|   PUT|Login|        null|     6071|null|   307|1538352405000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          137|    null|  null| paid|    null|   GET| Home|        null|     9422|null|   200|1538352461000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           68|    null|  null| paid|    null|   GET| Home|        null|    21447|null|   200|1538352464000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           69|    null|  null| paid|    null|   PUT|Login|        null|    21447|null|   307|1538352465000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          138|    null|  null| paid|    null|   GET| Home|        null|     9422|null|   200|1538352481000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          139|    null|  null| paid|    null|   PUT|Login|        null|     9422|null|   307|1538352482000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          279|    null|  null| paid|    null|   GET| Home|        null|     9659|null|   200|1538352501000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|           29|    null|  null| free|    null|   GET| Home|        null|    10467|null|   200|1538352507000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          150|    null|  null| paid|    null|   GET| Home|        null|    13098|null|   200|1538352520000|     null|1261737|\n",
      "|  null|Logged Out|     null|  null|          280|    null|  null| paid|    null|   GET| Home|        null|     9659|null|   200|1538352542000|     null|1261737|\n",
      "+------+----------+---------+------+-------------+--------+------+-----+--------+------+-----+------------+---------+----+------+-------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate / clean up registration==0/null rows\n",
    "# observation from the ydata report is that it is the same number of rows as the missing first/last names so likely the same \"issue\"\n",
    "df.where((f.col(\"registration\")<1) |f.col(\"registration\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|min(registration)|\n",
      "+-----------------+\n",
      "|    1508018725000|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.min(f.col(\"registration\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-check some results given don't fully trust the ydata report now:\n",
    "describe_pdf = df.describe().toPandas()\n",
    "describe_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out user 1261737\n",
    "df = df.where(f.col(\"userId\")!=1261737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Extraction\n",
    "\n",
    "As we have seen that most of the Sparkify users are on the \"paid\" level, I have decided to try to come up with a model that will predict users that are more likely to leave that level (either by downgrading to the free level or closing their account altogether) in the hope this will have biggest impact on maintaining future revenue.\n",
    "\n",
    "need to filter to just paid and flag those that downgraded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_ids = df.select(\"userID\").distinct()\n",
    "print(f\"There are {df_user_ids.count()} unique userIds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the dataframe down to just the interactions had whilst users are on the paid tier, using the \"phase\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"level_change\",\n",
    "    f.when(f.col(\"page\")==\"Submit Upgrade\", 1)\n",
    "    .when(f.col(\"page\")==\"Submit Downgrade\", -1)\n",
    "    .when(f.col(\"page\")==\"Cancellation Confirmation\", -1)\n",
    "    .otherwise(0))\n",
    "\n",
    "window = Window.partitionBy(\"userId\").orderBy(f.asc(\"ts\")).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"level_phase\", f.sum(\"level_change\").over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"userId\", \"page\", \"ts\", \"level_phase\").where(f.col(\"page\").isin([\"Submit Upgrade\", \"Submit Downgrade\", \"cancellation Confirmation\"])).sort(\"userId\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the raw input for user 1000214 as shouldn't have a downgrade first (-1's):\n",
    "raw_df.select(\"userId\", \"page\", \"ts\").where((f.col(\"page\").isin([\"Submit Upgrade\", \"Submit Downgrade\", \"cancellation Confirmation\"]))&(f.col(\"userId\")==1000214)).sort(\"userId\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((f.col(\"page\")==\"Submit Upgrade\")&(f.col(\"level_phase\")!=1)).select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So there are 926 users that had a downgrade / cancellation before an upgrade. sample ID's:\n",
    "df.where((f.col(\"page\")==\"Submit Upgrade\")&(f.col(\"level_phase\")!=1)).select(\"userId\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check a second sample\n",
    "raw_df.select(\"userId\", \"page\", \"ts\").where((f.col(\"page\").isin([\"Submit Upgrade\", \"Submit Downgrade\", \"cancellation Confirmation\"]))&(f.col(\"userId\")==1250440)).sort(\"userId\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see users that downgrade first, implying we must be missing earlier data which covers what they were doing as a paid user.\n",
    "We want to know all the actions of paying users, missing interactions could be important in predicting churn, so \"paying periods\" with missing data will be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up: For all users that upgraded, remove any data before their first upgrade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "      .withColumn(\"upgrade_flag\", f.when(f.col(\"page\")==\"Submit Upgrade\", 1).otherwise(0))\n",
    "      .withColumn(\"upgrade_phase\", f.sum(\"upgrade_flag\").over(window))\n",
    "      .where(f.col(\"upgrade_phase\")>0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-calculate level change and level phase cols with \"cleaned\" data:\n",
    "df = df.withColumn(\"level_change\",\n",
    "    f.when(f.col(\"page\")==\"Submit Upgrade\", 1)\n",
    "    .when(f.col(\"page\")==\"Submit Downgrade\", -1)\n",
    "    .when(f.col(\"page\")==\"Cancellation Confirmation\", -1)\n",
    "    .otherwise(0))\n",
    "\n",
    "window = Window.partitionBy(\"userId\").orderBy(f.asc(\"ts\")).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"level_phase\", f.sum(\"level_change\").over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"userId\", \"page\", \"ts\", \"level_phase\").where(f.col(\"page\").isin([\"Submit Upgrade\", \"Submit Downgrade\", \"cancellation Confirmation\"])).sort(\"userId\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check max phase is 1\n",
    "# check min phase is -1, for if downgrade and then cancel\n",
    "# check always upgrade first, i.e. phase is 1 when do an upgrade\n",
    "df.select(f.max(\"level_phase\"), f.min(\"level_phase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((f.col(\"page\")==\"Submit Upgrade\")&(f.col(\"level_phase\")!=1)).select(\"userId\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in samples users have upgraded multiple times (with downgrades in-between), we want to be able to differentiate between each period they were a paying user, and determine for each period if they churned or not, and hopefully use the features of each in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .withColumn(\"userId_paid_phase\", f.concat_ws(\"_\", f.col(\"userId\"), f.col(\"upgrade_phase\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_window = Window.partitionBy(\"userID\",\"upgrade_phase\")\n",
    "df = df.withColumn(\"downgraded\", f.min(\"level_change\").over(downgrade_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique User ID's in data:\")\n",
    "raw_df.select(f.countDistinct(\"userId\")).show()\n",
    "print(\"Number of unique User ID's that were on a paid level at some point:\")\n",
    "raw_df.select(\"userId\",\"page\").where(f.col(\"page\")==\"Submit Upgrade\").select(f.countDistinct(\"userId\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12082/22278 * 100 #% of users that paid level at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review interactions where level changes for any data that might need cleaning\n",
    "df.where(f.col(\"level_change\")!=0).sort(\"userId\").select(\"page\", \"userId\", \"level\", \"level_change\",\"level_phase\",\"upgrade_flag\",\"upgrade_phase\",\"userId_paid_phase\",\"downgraded\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"downgraded\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wasn't expecting +1, would indicate there is a user that has upgraded their account and done nothing else since? investigate:\n",
    "df.where(f.col(\"downgraded\")==1).select(\"ts\", \"page\", \"userId\", \"level\", \"level_change\",\"level_phase\",\"upgrade_flag\",\"upgrade_phase\",\"userId_paid_phase\",\"downgraded\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_raw data for this user:\n",
    "raw_df.where((f.col(\"userId\")==1977992)&(f.col(\"ts\")>=1543620507000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO indeed that is the case, data and analysis is correct. Could think about filtering brand new users out of the data as not enough information to predict on churn.\n",
    "df = df.withColumn(\"downgraded\", f.when(f.col(\"downgraded\")==-1,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove data that is not for \"paying\" users - i.e data when level was free, or \"level_phase\" < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(f.col(\"level_phase\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataframe we have so far:\n",
    "# number of unique times a user has had a period they have been a \"paying\" customer\n",
    "# number of those periods where users have churned back to free\n",
    "feature_df = df.groupBy(\"userId_paid_phase\").agg(f.max(\"downgraded\").alias(\"churned\"), f.max(\"userId\").alias(\"userId\"))\n",
    "feature_df.show()\n",
    "\n",
    "#f.countDistinct(\"userId\").alias(\"distinct_count_userId\")).sort(f.desc(\"distinct_count_userId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of \"paying periods\" in data: {feature_df.count()}')\n",
    "print(f'Number of \"churns\" from paid to free/cancelled in data: {feature_df.where(f.col(\"churned\")==1).count()}')\n",
    "print(f'Number of paying periods that didn\\'t churn: {feature_df.where(f.col(\"churned\")==0).count()} ')\n",
    "print(f'Number of unique users in the data: {feature_df.select(\"userId\").distinct().count()}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.groupBy(\"userId\").agg(f.count(\"userId\").alias(\"distinct_count_userId\"), f.min(\"churned\")).sort(f.desc(\"distinct_count_userId\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so one user has upgraded at least 8 times, and in our data are \"currently\" paying users. Maybe how many times they have downgraded before could also be a factor\n",
    "raw_df.select(\"userId\", \"page\", \"ts\").where((f.col(\"page\").isin([\"Submit Upgrade\", \"Submit Downgrade\", \"cancellation Confirmation\"]))&(f.col(\"userId\")==1662781)).sort(\"userId\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence our goal is to predict which of the 7617 paying users we have are next to downgrade / cancel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction \n",
    "I first brainstorm possible features to investigate:\n",
    "\n",
    " - Duration of being at the paid level\n",
    " - Number of unique artists listened  (ever / over a period)\n",
    " - gender column\n",
    " - itemInSession column\n",
    " - Many from the pages column:\n",
    "    - number of songs played (ever / over a period)\n",
    "    - number of thumbs up (ever / over a period) \n",
    "    - number of thumbs down (ever / over a period)\n",
    "    - number of visits to the downgrade and or cancel page (ever / over a period)\n",
    "    - number of visits to the help page\n",
    "    - number of HTTP errors (404 codes)\n",
    "\n",
    " - Number of times the user has churned previously\n",
    "\n",
    " I will then look at these to see if a model might be able to use them to differentiate between users about\n",
    " to downgrade and those that are not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration at the paid level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"level_start_ts\",f.min(f.col(\"ts\")).over(Window.partitionBy(\"userId_paid_phase\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"last_ts\", f.max(f.col(\"ts\")).over(Window.partitionBy(\"userId_paid_phase\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"paid_duration_s\", (f.col(\"last_ts\") - f.col(\"level_start_ts\"))/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_durations = df.select(\"userId_paid_phase\", \"paid_duration_s\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_durations.count() == feature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_durations.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.join(df_durations, on=\"userId_paid_phase\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf = feature_df.toPandas()\n",
    "feature_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=feature_pdf, x=\"churned\", y=\"paid_duration_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf[\"paid_duration_days\"] = feature_pdf.paid_duration_s/(86400)\n",
    "sns.violinplot(data=feature_pdf, x=\"churned\", y=\"paid_duration_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf[\"paid_duration_days\"].mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check that the large number of users that upgraded recently is correct?\n",
    "tdf = feat_pdf.copy(deep=True)\n",
    "tdf = tdf.sort_values(\"paid_duration_s\", ascending=True)\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.where(f.col(\"userId\")==1814132).sort(\"ts\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_artists = df.groupby(\"userId_paid_phase\").agg(f.countDistinct(\"artist\").alias(\"unique_artist_count\"))\n",
    "feature_df = feature_df.join(df_unique_artists, on=\"userId_paid_phase\", how=\"left\")\n",
    "feature_df.sort(\"unique_artist_count\").show(5)\n",
    "feature_df.sort(f.desc(\"unique_artist_count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results\n",
    "feature_pdf = feature_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf.groupby(\"churned\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf[\"dummy\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=feature_pdf,y=\"unique_artist_count\", x=\"dummy\", hue=\"churned\", split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = df.withColumn(\"gender_encoded_male_1\",f.when(f.col(\"gender\")==\"M\",1).otherwise(0)).groupby(\"userId_paid_phase\").agg(f.max(\"gender_encoded_male_1\").alias(\"male_1_female_0\"))\n",
    "feature_df = feature_df.join(df_gender, on=\"userId_paid_phase\", how=\"left\")\n",
    "feature_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf = feature_df.toPandas()\n",
    "feature_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the percentage of each gender that churn?\n",
    "gender_summary_pdf = feature_pdf.groupby(\"male_1_female_0\").apply(lambda x: pd.Series({\"total_count\":x[\"churned\"].count(), \"churned_count\":x[\"churned\"].sum()}))\n",
    "gender_summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_summary_pdf[\"pct_churn\"] = gender_summary_pdf.churned_count / gender_summary_pdf.total_count * 100\n",
    "gender_summary_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3611+3907)/(7246+7889)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of users that have churned is 49.7%, and that varies very minimally for different genders. It is tempting to omit gender as a feature, but it could be that when combined with other features it is useful in making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itemIn Session Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how this column works:\n",
    "df.sort(\"userId_paid_phase\").select(\"userId\", \"userId_paid_phase\", \"ts\", \"itemInSession\", \"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see each row increments the itemInSession count by 1, wheras I am interested in the total\n",
    "# number of items in each session, i.e. the \"peaks\" of the values before they drop down again.\n",
    "w = Window.partitionBy(\"userId_paid_phase\").orderBy(\"ts\")\n",
    "last_row_window = Window.partitionBy(\"userId_paid_phase\",).orderBy(f.desc('ts'))\n",
    "\n",
    "df_max_items_in_sessions = (\n",
    "    df\n",
    "    .sort(\"userId_paid_phase\")\n",
    "    .select(\"userId\", \"userId_paid_phase\", \"ts\", \"itemInSession\", \"page\")\n",
    "    .withColumn(\"prev_item\",f.lag(\"itemInSession\").over(w))\n",
    "    .withColumn(\"item_minus_prev\",f.col(\"itemInSession\")-f.col(\"prev_item\"))\n",
    "    .withColumn(\"row_num\", f.row_number().over(last_row_window))\n",
    "    .where((f.col(\"item_minus_prev\")<1) | (f.col(\"row_num\")==1))\n",
    "    .select(\"userId_paid_phase\",f.col(\"prev_item\").alias(\"items_in_session_counts\"))\n",
    ")\n",
    "df_max_items_in_sessions.show(5)\n",
    "\n",
    "#.where(f.col(\"item_minus_prev\")<1).select(\"\",f.col(\"prev_item\").alias(\"session_peaks\")).show()\n",
    "\n",
    "# Note, the count for the last row is off by 1 (as we take the previous row value) but should\n",
    "# be good enough as these numbers are comparitvely large and have spent excessive time already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also need to keep last row per paid period?\n",
    "df_max_items_in_sessions.where(f.col(\"userId_paid_phase\")==\"1999908_1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(f.col(\"userId_paid_phase\")==\"1999908_1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible improvement option to investigate in the future - if should \"reset\" the item count at the start of a paid session, or keep both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itemInSession = df_max_items_in_sessions.groupby(\"userId_paid_phase\").agg(f.mean(\"items_in_session_counts\").alias(\"mean(items_in_session_counts)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.join(df_itemInSession, on=\"userId_paid_phase\", how=\"left\")\n",
    "feature_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pdf = feature_df.toPandas()\n",
    "feature_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=feature_pdf,y=\"mean(items_in_session_counts)\",x='churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review various columns looking at total count, and average past week compared to overall average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_songs_played = (\n",
    "#     df\n",
    "#     .where(f.col(\"page\")==\"NextSong\")\n",
    "#     .groupby(\"userId_paid_phase\").agg(f.count(\"artist\").alias(\"total_song_count\"))\n",
    "# )\n",
    "# df_songs_played.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Future improvement - It sounds reasonable that some of the churned users could be using the service less than they used to,\n",
    "# so would like to try and generate a feature that compares the number of songs played recently, to the \n",
    "# number of songs played in the past. Ideally need to know when \"now\" is to calculate features like:\n",
    "# \"average songs per day last week vs average songs per day since upgrading\"\n",
    "\n",
    "# As we are not sure when \"now\" is, just look at the previous 1 week before either churning or the last interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_week_in_ms = 1000 * 60 * 60 * 24 * 7\n",
    "df = df.withColumn(\"one_week_prior_ts\", f.col(\"last_ts\") - one_week_in_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df = feature_df.drop('page_NextSong_total_count',\n",
    "#  'page_NextSong_last_week_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_values_to_count = [\n",
    "    {\"c\":\"page\", \"v\":\"NextSong\"},\n",
    "    {\"c\":\"page\", \"v\":\"Thumbs Up\"},\n",
    "    {\"c\":\"page\", \"v\":\"Thumbs Down\"},\n",
    "    {\"c\":\"page\", \"v\":\"Downgrade\"},\n",
    "    {\"c\":\"page\", \"v\":\"Cancel\"},\n",
    "    {\"c\":\"page\", \"v\":\"Help\"},\n",
    "    {\"c\":\"status\", \"v\":404},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_value_to_count(input_df, col, value,feature_df):\n",
    "    \"\"\"calculate a new feature using the col and value field \n",
    "    on the input_df, and return the feature_df with a new column\n",
    "    Also plot the results for review\"\"\"\n",
    "    \n",
    "    new_feature_total_col_name = f\"{col}_{value}_total_count\"\n",
    "    new_feature_last_week_count_col_name = f\"{col}_{value}_last_week_count\"\n",
    "    new_feature_last_week_pct_col_name = f\"{col}_{value}_last_week_to_total_rate_ratio_pct\"\n",
    "    \n",
    "    new_feature_df = (\n",
    "        input_df\n",
    "        .where(f.col(col)==value).groupby(\"userId_paid_phase\")\n",
    "        .agg(\n",
    "            f.count(col).alias(new_feature_total_col_name),\n",
    "            f.max(\"paid_duration_s\").alias(\"paid_duration_s\")\n",
    "        )   \n",
    "    )\n",
    "    \n",
    "    last_week_df = (\n",
    "        input_df.where(f.col(\"ts\")>=f.col(\"one_week_prior_ts\"))\n",
    "        .where(f.col(col)==value).groupby(\"userId_paid_phase\")\n",
    "        .agg(f.count(col).alias(new_feature_last_week_count_col_name))\n",
    "    )\n",
    "    new_feature_df = new_feature_df.join(last_week_df, on=\"userId_paid_phase\", how=\"left\")\n",
    "    new_feature_df = (\n",
    "        new_feature_df\n",
    "        .withColumn(new_feature_last_week_pct_col_name,\n",
    "                    ((f.col(new_feature_last_week_count_col_name))/(604800))\n",
    "                    /\n",
    "                    ((f.col(new_feature_total_col_name))/(f.col(\"paid_duration_s\")))\n",
    "                    *100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    feature_df = feature_df.join(new_feature_df.drop(\"paid_duration_s\"), on=\"userId_paid_phase\", how=\"left\")\n",
    "    feature_df = feature_df.fillna(0, subset=[new_feature_total_col_name,new_feature_last_week_pct_col_name])\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_v_dict in col_values_to_count:\n",
    "    col = c_v_dict[\"c\"]\n",
    "    value = c_v_dict[\"v\"]\n",
    "    print(f\"\\nProcessing count of value {value} in column {col} ...\")\n",
    "    feature_df = process_value_to_count(input_df=df,col=col,value=value, feature_df=feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pdf = feature_df.toPandas()\n",
    "feat_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"page\"\n",
    "value = \"NextSong\"\n",
    "new_feature_total_col_name = f\"{col}_{value}_total_count\"\n",
    "new_feature_last_week_pct_col_name = f\"{col}_{value}_last_week_to_total_rate_ratio_pct\"\n",
    "\n",
    "plt.figure()\n",
    "fig, (bp,vp) = plt.subplots(1,2, sharey=True)\n",
    "\n",
    "fig.suptitle(f\"Plots for total counts of {value} in column {col}\")\n",
    "sns.boxplot(ax=bp, data=feat_pdf, y=new_feature_total_col_name, x=\"churned\")\n",
    "\n",
    "plt.figure()\n",
    "sns.violinplot(ax=vp, data=feat_pdf, y=new_feature_total_col_name, x=\"churned\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "fig, (bp,vp) = plt.subplots(1,2, sharey=True)\n",
    "\n",
    "fig.suptitle(f\"Plots for % of last week vs total of {value} in column {col}\")\n",
    "sns.boxplot(ax=bp, data=feat_pdf, y=new_feature_last_week_pct_col_name, x=\"churned\")\n",
    "\n",
    "plt.figure()\n",
    "sns.violinplot(ax=vp, data=feat_pdf, y=new_feature_last_week_pct_col_name, x=\"churned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot New Features\n",
    "def display_plots(pdf, list_of_col_val_dict):\n",
    "    f_pdf = pdf\n",
    "    \n",
    "    for c_v_dict in list_of_col_val_dict:\n",
    "        col = c_v_dict[\"c\"]\n",
    "        value = c_v_dict[\"v\"]\n",
    "        print(f\"\\nDisplaying visuals for value {value} in column {col} ...\")\n",
    "        new_feature_total_col_name = f\"{col}_{value}_total_count\"\n",
    "        new_feature_last_week_pct_col_name = f\"{col}_{value}_last_week_to_total_rate_ratio_pct\"\n",
    "    \n",
    "        plt.figure()\n",
    "        fig, (bp,vp) = plt.subplots(1,2, sharey=True)\n",
    "        \n",
    "        fig.suptitle(f\"Plots for total counts of {value} in column {col}\")\n",
    "        sns.boxplot(ax=bp, data=f_pdf, y=new_feature_total_col_name, x=\"churned\")\n",
    "        \n",
    "        plt.figure()\n",
    "        sns.violinplot(ax=vp, data=f_pdf, y=new_feature_total_col_name, x=\"churned\")\n",
    "        \n",
    "        \n",
    "        plt.figure()\n",
    "        fig, (bp,vp) = plt.subplots(1,2)\n",
    "        \n",
    "        fig.suptitle(f\"Plots for % of last week vs total of {value} in column {col}\")\n",
    "        sns.boxplot(ax=bp, data=f_pdf, y=new_feature_last_week_pct_col_name, x=\"churned\")\n",
    "        \n",
    "        plt.figure()\n",
    "        sns.violinplot(ax=vp, data=f_pdf, y=new_feature_last_week_pct_col_name, x=\"churned\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display_plots(feat_pdf, col_values_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "\n",
    "- visits to the cancel page - all users that don't churn have never gone to the cancel page. Whilst some of the users that churn don't visit the page, that could also be because they only downgrade and don't cancel their accounts. We are not sure what happens on this interaction, it seems like you only reach that page if you have churned already, so will be removed from the data.\n",
    "(Otherwise the model will use the fact that never got to the cancel page to predict no churn, but in reality that is too late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.drop('page_Cancel_last_week_count',\n",
    " 'page_Cancel_last_week_pct',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible feature improvements - look at percentages of each item against other values as well as the total counts. Again also compare \"recent\" counts/percentages to overall counts/percentages to try and detect a change in behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Model to predict Churn\n",
    "\n",
    "As we are trying to determine if a \"user paying period\" falls into a category of churned / not churned, we have a __classification__ problem to solve.\n",
    "\n",
    "Supported classification algorithms in spark.ml include:\n",
    "- Logistic Regression\n",
    "- Random Forests - _chosen to try on our data_\n",
    "- Gradient-Boosted Trees\n",
    "- Support Vector Machines - _chosen to try on our data_\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no missing values\n",
    "feature_df.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in feature_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fill missing values with 0, which is where the user didn't have any of those events, e.g. never went to Help page etc.\n",
    "feature_df = feature_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"features_df has {feature_df.count()} rows and {len(feature_df.columns)} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = feature_df.columns[3:]\n",
    "target_col = \"churned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.schema[\"userId\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [c for c in input_cols if c!=\"male_1_female_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical cols to vectors:\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"numerical_features_vector\")\n",
    "feature_df = assembler.transform(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"numerical_features_vector\", outputCol=\"scaled_numerical_features_vector\", withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(feature_df)\n",
    "feature_df = scalerModel.transform(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back the gender input col:\n",
    "assembler = VectorAssembler(inputCols=[\"scaled_numerical_features_vector\",\"male_1_female_0\"], outputCol=\"features_vec\")\n",
    "feature_df = assembler.transform(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = feature_df.select(f.col(\"features_vec\").alias(\"x\"), f.col(\"churned\").alias(\"Y\"))\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp cell to speed things up... re run one abovr when remove this\n",
    "model_df,_ = model_df.randomSplit([0.1,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_df.randomSplit([0.8, 0.2], seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We saw previously close to 50% of \"user paid periods\" churned, so if we were to randomly guess if that period churned with an equal probability (50%), we should achieve an accuracy of 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Y\", \n",
    "                            featuresCol=\"x\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"pred\") # prediction\n",
    "\n",
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, [5,10])\n",
    "    .addGrid(rf.maxBins, [25,35])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "pipeline=Pipeline(stages=[rf])\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=BinaryClassificationEvaluator(labelCol=\"Y\", ),\n",
    "    numFolds=2\n",
    ")\n",
    "\n",
    "rf_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = rf_model\n",
    "y_pred = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "evaluator.setLabelCol(\"Y\")\n",
    "auc = evaluator.evaluate(y_pred)\n",
    "\n",
    "print(f\"\\t{auc=}\")\n",
    "\n",
    "print(f\"Best model params: {model.bestModel.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test):\n",
    "    model\n",
    "    y_pred = model.transform(test)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "    evaluator.setLabelCol(\"Y\")\n",
    "    auc = evaluator.evaluate(y_pred)\n",
    "\n",
    "    print(f\"\\t{auc=}\")\n",
    "\n",
    "    print(f\"Best model params: {model.bestModel.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(labelCol=\"Y\", featuresCol=\"x\")\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "svc_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(svc.aggregationDepth, [2,3])\n",
    "                 .build())\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=svc_param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2)\n",
    "\n",
    "svc_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(svc_model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run models with input (if time allows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = feature_df.select(f.col(\"features_vec\").alias(\"x\"), f.col(\"churned\").alias(\"Y\"))\n",
    "\n",
    "train, test = model_df.randomSplit([0.8, 0.2], seed=77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Y\", \n",
    "                            featuresCol=\"x\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\") # prediction\n",
    "\n",
    "rf_param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, [5,10])\n",
    "    .addGrid(rf.maxBins, [25,35])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "pipeline=Pipeline(stages=[rf])\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=rf_param_grid,\n",
    "    evaluator=BinaryClassificationEvaluator(labelCol=\"Y\", ),\n",
    "    numFolds=2\n",
    ")\n",
    "\n",
    "rf_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(labelCol=\"Y\", featuresCol=\"x\")\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "svc_param_grid = (ParamGridBuilder()\n",
    "                 .addGrid(svc.aggregationDepth, [2,3])\n",
    "                 .build())\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=svc_param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2)\n",
    "\n",
    "svc_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(svc_model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "- check / deal with users that might have upgraded multiple times\n",
    "- Tune the period of \"recent\" activity to compare overall with, value different to 1 week could be better. Could also be a hyper parameter for the grid search in the pipeline. Adding the transform steps teo the pipeline also.\n",
    "- Other feature ideas:\n",
    "    - number of sessions (ever / over a period)\n",
    "    - number of friends (ever / over a period) \n",
    "    - duration since registration\n",
    "    - duration since upgrading to paid level\n",
    "    -  day of the week / the week of the year?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
